callback: callback_npmi
callback_npmi: { embedding_path: '', every: 100, metric: npmi, topk: 100 }
callback_wetc: { embedding_path: '', every: 100, metric: wetc, topk: 10 }
comment: ''
dataset: { batch_size: 64, data_dir: data/cnndm/corpus, dev_ratio: 0.1, device: device }
device: cpu
drop_rate: 0.2
hidden:
  activate: Softplus
  features: [ 0, 50, 50 ]
  type: models.utils.get_mlp
normal:
  in_features: !!python/tuple [ hidden, features, -1 ]
  out_features: 50
  type: models.utils.NormalParameter
gsm:
  h_to_z:
    type: models.utils.Sequential
    args:
      type: collections.OrderedDict
      f1:
        in_features: !!python/tuple [ normal, out_features ]
        out_features: !!python/tuple [ gsmtopic, k ]
        type: torch.nn.Linear
      f2:
        type: torch.nn.Softmax
        dim: -1
      f3:
        type: torch.nn.Dropout
        p: drop_rate
  hidden:
    type: models.utils.Sequential
    args:
      type: collections.OrderedDict
      f1: hidden
      f2:
        type: torch.nn.Dropout
        p: drop_rate
  normal: normal
  topics: gsmtopic
  penalty: 50
  type: models.NTM.GSM
gsmtopic:
  type: models.utils.EmbTopic
  k: 50
  embedding:
    type: mlutils.pt.modules.get_embedding
    num_embeddings: !!python/tuple [ hidden, features, 0 ]
    embedding_dim: 50
GSMtrainer:
  # early_stop: 50
  eval_metric: { key: ppx_doc, type: mlutils.pt.training.eval_metric_cmp_key }
  evaluate_interval: 10
  num_checkpoints_keep: 1
  num_epochs: 12000
  statistics: { dotted_path: experiments.utils.PerplexStatistics, type: mlutils.pt.training.parse_class }
  trainer_batch: trainer_batch
  type: mlutils.pt.training.GSMTrainer
trainer_batch:
  device: device
  loss: sum
  model: gsm
  optimizer: { lr: 0.00005, betas: [ 0.99, 0.999 ], type: torch.optim.Adam }
  test_sample: 100
  type: experiments.utils.BatchOperation
